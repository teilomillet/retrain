# sepa-semantic-30b: Does SEPA separate from baseline on a larger model?
#
# Same design as sepa-semantic but on Qwen3-30B-A3B (MoE, 3B active).
# Full 3-seed experiment to test the +0.6pp hint from the single-seed probe.
#
# 2 conditions x 3 seeds x 100 steps = 6 runs
#
# Run: retrain campaigns/sepa-semantic-30b.toml

[campaign]
seeds = [42, 101, 202]
max_steps = 100
parallel = true
max_workers = 6
stagger_seconds = 12

[[campaign.conditions]]
advantage_mode = "grpo"
transform_mode = "none"

[[campaign.conditions]]
advantage_mode = "maxrl"
transform_mode = "gtpo_sepa"

[backend]
backend = "tinker"

[model]
model = "Qwen/Qwen3-30B-A3B-Instruct-2507"
lora_rank = 64

[training]
batch_size = 8
group_size = 16
max_tokens = 2048
temperature = 0.7
lr = 4e-5
save_every = 20

[planning]
detector = "semantic"

[sepa]
steps = 100
schedule = "linear"
delay_steps = 5

[squeeze]
min_variance_retention = 0.95

[logging]
wandb_project = "sepa-semantic-30b"
