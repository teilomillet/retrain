# Yue et al. entropy masking — 30B model replication
# Same as entropy-mask-yue.toml but on Qwen3-30B-A3B (MoE, 3B active)
# Tests whether surprisal-based masking improves with scale.

[campaign]
seeds = [42, 101, 202]
max_steps = 100
parallel = true
max_workers = 3

[[campaign.conditions]]
advantage_mode = "maxrl"
transform_mode = "entropy_mask"

[backend]
backend = "tinker"

[model]
model = "Qwen/Qwen3-30B-A3B-Instruct-2507"
lora_rank = 64

[training]
batch_size = 8
group_size = 16
max_tokens = 2048
temperature = 0.7
lr = 4e-5
save_every = 20

[algorithm]
entropy_mask_rho = 0.2

[gtpo]
beta = 0.0    # No GTPO weighting — pure masking (faithful to Yue et al.)

[squeeze]
min_variance_retention = 0.95

[logging]
wandb_project = "entropy-mask-yue-30b"
