from typing import TypedDict, Dict, Any, List, Tuple, Optional, TYPE_CHECKING
import torch

# Use TYPE_CHECKING to avoid circular import
if TYPE_CHECKING:
    from retrain.environment.types import LLMAction, EnvironmentObservation
else:
    # For runtime, we'll use Any to avoid the circular import
    LLMAction = Any
    EnvironmentObservation = Any

class SingleTurnDataForTrainer(TypedDict):
    """
    Data structure representing a single processed turn, ready for the RL trainer.
    """
    prompt_text: str              # The text prompt given to the LLM for this turn
    completion_text: str          # The raw text completion generated by the LLM for this turn
    final_combined_reward: float  # The final scalar reward for this turn after all calculations
    
    # Optional: For TRL, this 'example' can carry more data if needed by PPOData or custom collators
    # It could include parts of the environment's 'info' dict from that step,
    # or specific metrics generated during reward calculation.
    auxiliary_data: Dict[str, Any]
    old_per_token_logps: Optional[torch.Tensor] # Log probabilities of the completion_text tokens

# Type alias for what SmolAgentEnv.rollout returns / FastMCPEnv will return
# The old_per_token_logps are now part of each LLMAction in executed_llm_actions.
class RawRolloutData(TypedDict):
    full_conversation_history: List[Dict[str, str]]
    executed_llm_actions: List[LLMAction] # Each LLMAction now contains its own old_per_token_logps
    intrinsic_rewards_per_turn: List[float]
    final_environment_observation: EnvironmentObservation
    step_info_dicts_per_turn: List[Dict[str, Any]]
    rollout_metadata: Dict[str, Any] # General metadata about the rollout

ProcessedTrajectory = List[SingleTurnDataForTrainer] 

# Added RewardFunctionConfig here
class RewardFunctionConfig(TypedDict, total=False):
    name: str # Name of the reward function in the registry, often implicitly the key in a dict
    weight: float # Weight for this reward function when combining multiple rewards
    params: Dict[str, Any] # Parameters specific to the base reward function
    verifiers: Optional[List[str]] # List of verifier names (from verifier registry) to apply
    verifier_penalty: Optional[float] # Penalty to apply if any verifier fails. If None, might use a default or skip.
    # For rollout rewards, a distribution strategy might be specified:
    distribution_strategy: Optional[str] # e.g., "last_step", "all_steps_average", "all_steps_equal" 