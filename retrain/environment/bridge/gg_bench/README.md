# `retrain.environment.bridge.gg_bench`

**Purpose**: This bridge enables `retrain` to use game environments generated by the [GG-Bench project](https://arxiv.org/abs/2405.07215) for model retraining.

`gg-bench` generates two-player, turn-based strategy games. These games are implemented as standard synchronous Python environments compatible with the Gymnasium (formerly Gym) API.

## Key Components

-   **`loader.py`**: Defines the `load_gg_bench_env` function.
    -   This function is responsible for locating a specific `gg-bench` game file (e.g., `env_123.py`) based on a game ID and path.
    -   It dynamically loads the `CustomEnv` class from this file.
    -   Crucially, it then applies `gg-bench`'s own standard wrappers: `TimeoutEnv`, `MetadataEnv`, and `AlternatingAgentEnv`. The `AlternatingAgentEnv` is essential as it manages the two-player turn logic and opponent interactions, which is leveraged by `retrain` for self-play scenarios.

-   **`wrapper.py`**: Defines the `AsyncGGBenchBridgeWrapper` class.
    -   This class takes a fully wrapped synchronous `gg-bench` environment (i.e., the output from `load_gg_bench_env` before this final wrap).
    -   It adapts this synchronous environment to `retrain`'s asynchronous `Environment` interface (as defined in `retrain.environment.Environment`). This involves using `asyncio.to_thread` or `loop.run_in_executor` to call the synchronous `step` and `reset` methods of the `gg-bench` environment without blocking `retrain`'s event loop.
    -   It also exposes asynchronous versions of control methods from `AlternatingAgentEnv` (like `add_opposing_agent` and `set_epsilon`) so that `retrain`'s training orchestrator can manage self-play dynamics.

## How It Works for Retraining

1.  `retrain`'s training system (e.g., a `Trainer` component) calls `load_gg_bench_env` to get an instance of a specific `gg-bench` game.
2.  This instance is an `AsyncGGBenchBridgeWrapper`, which conforms to `retrain`'s `Environment` API.
3.  The `Trainer` can then interact with the game using `await env.step()` and `await env.reset()`.
4.  For self-play, the `Trainer` uses methods like `await env.add_opposing_agent()` to provide the `AlternatingAgentEnv` (deep within the wrapper stack) with the policies of opponent agents (e.g., past versions of the agent being trained).

## Dependencies

-   This bridge assumes that the `gg-bench` library is installed in the Python environment and that its utility wrappers (`gg_bench.utils.env_wrappers`) are importable.
-   It also requires the path to the directory where `gg-bench` has saved its generated environment `.py` files. 