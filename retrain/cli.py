"""Single entry point for retrain.

Usage:
    retrain                  # loads retrain.toml from cwd
    retrain config.toml      # single training run
    retrain campaign.toml    # campaign (if TOML has [campaign] section)
    retrain init             # generate a starter retrain.toml
    retrain doctor           # check installed dependencies for all components
    retrain man              # human/agent-friendly manual
    retrain --seed 42 --lr 1e-4   # override config values from CLI

A TOML with a [campaign] section runs multiple conditions × seeds.
A TOML without it runs a single training job. Same command either way.
"""

from __future__ import annotations

import json
import os
import re
import sys
import tomllib
from pathlib import Path


_STARTER_TOML = """\
# retrain configuration — generated by `retrain init`
# Docs: docs/configuration.md

[model]
model = "Qwen/Qwen3-4B-Instruct-2507"
lora_rank = 32

[algorithm]
advantage_mode = "maxrl"     # grpo | maxrl
transform_mode = "gtpo_sepa" # none | gtpo | gtpo_hicra | gtpo_sepa

[training]
seed = -1                    # -1 = no seed
max_steps = 100              # conservative default for first run
batch_size = 4               # conservative default for first run
group_size = 16
max_tokens = 2048
temperature = 0.7
lr = 4e-5
save_every = 20

[backend]
backend = "local"
adapter_path = "adapters/my_run"

[logging]
log_dir = "logs/train"
# wandb_project = ""         # uncomment to enable wandb
"""


_QUICKSTART_TOML = """\
# retrain quickstart — 20-step smoke test
# Generated by `retrain init --template quickstart`

[model]
model = "Qwen/Qwen3-4B-Instruct-2507"
lora_rank = 32

[algorithm]
advantage_mode = "grpo"
transform_mode = "none"

[training]
seed = -1
max_steps = 20
batch_size = 2
group_size = 8
max_tokens = 1024
temperature = 0.7
lr = 4e-5
save_every = 0               # no checkpoints for a quick test

[backend]
backend = "local"
adapter_path = "adapters/quickstart"

[logging]
log_dir = "logs/quickstart"
"""

_EXPERIMENT_TOML = """\
# retrain experiment — reproducible 500-step training run
# Generated by `retrain init --template experiment`

[model]
model = "Qwen/Qwen3-4B-Instruct-2507"
lora_rank = 32

[algorithm]
advantage_mode = "maxrl"
transform_mode = "gtpo_sepa"

[training]
seed = 42
max_steps = 500
batch_size = 8
group_size = 16
max_tokens = 2048
temperature = 0.7
lr = 4e-5
save_every = 50

[sepa]
steps = 500
schedule = "linear"
delay_steps = 50
correct_rate_gate = 0.1

[backend]
backend = "local"
adapter_path = "adapters/experiment"

[logging]
log_dir = "logs/experiment"
wandb_project = ""           # set your project name to enable wandb
"""

_CAMPAIGN_TOML = """\
# retrain campaign — sweep across conditions and seeds
# Generated by `retrain init --template campaign`

[campaign]
seeds = [42, 101, 202, 303]
max_steps = 200

[[campaign.conditions]]
advantage_mode = "grpo"
transform_mode = "none"

[[campaign.conditions]]
advantage_mode = "maxrl"
transform_mode = "gtpo_sepa"

# Base config shared by all runs
[model]
model = "Qwen/Qwen3-4B-Instruct-2507"
lora_rank = 32

[training]
batch_size = 8
group_size = 16
max_tokens = 2048
temperature = 0.7
lr = 4e-5

[backend]
backend = "local"
adapter_path = "adapters/campaign"

[logging]
log_dir = "logs/campaign"
# wandb_project = ""         # uncomment to enable wandb
"""

# name -> (content, filename)
_INIT_TEMPLATES: dict[str, tuple[str, str]] = {
    "default": (_STARTER_TOML, "retrain.toml"),
    "quickstart": (_QUICKSTART_TOML, "retrain.toml"),
    "experiment": (_EXPERIMENT_TOML, "retrain.toml"),
    "campaign": (_CAMPAIGN_TOML, "campaign.toml"),
}


_TOPIC_TO_SECTION = {
    "quickstart": "QUICKSTART",
    "environment": "ENVIRONMENT",
    "environments": "ENVIRONMENT",
    "troubleshooting": "TROUBLESHOOTING",
    "commands": "COMMANDS",
    "options": "OPTIONS",
    "configuration": "CONFIGURATION",
    "campaign": "CAMPAIGN MODE",
    "squeeze": "SQUEEZE MODE",
    "inference": "INFERENCE ENGINES",
    "validation": "VALIDATION",
    "diff": "DIFF MODE",
    "logging": "LOGGING",
    "examples": "EXAMPLES",
    "advantages": "ADVANTAGE PIPELINE",
    "metrics": "METRICS GUIDE",
    "architecture": "ARCHITECTURE",
    "files": "FILES",
    "plugins": "PLUGINS",
    "glossary": "GLOSSARY",
}

_AUTO_BLOCK_NAMES = (
    "COMMANDS",
    "OPTIONS",
    "QUICKSTART",
    "ENVIRONMENT",
)


def _print_top_help(cli_name: str) -> None:
    """Print concise top-level help with strong manual discoverability."""
    print(f"{cli_name} — TOML-first RLVR trainer")
    print()
    print("Usage:")
    print(f"  {cli_name} [config.toml] [--flag value ...]")
    print(f"  {cli_name} doctor")
    print(f"  {cli_name} init [--template NAME] [--list] [--interactive]")
    print(f"  {cli_name} status [logdir] [--json]")
    print(f"  {cli_name} explain [config.toml] [--json]")
    print(f"  {cli_name} diff <run_a> <run_b> [--json]")
    print(f"  {cli_name} man")
    print()
    print("Manual:")
    print(f"  {cli_name} man")
    print(f"  {cli_name} man --topic quickstart")
    print(f"  {cli_name} man --path")
    print(f"  {cli_name} man --sync")
    print(f"  {cli_name} man --check")
    print()
    print("Tip: read docs/configuration.md for full TOML reference.")


def _resolve_cli_name() -> str:
    """Best-effort CLI binary name for help text."""
    name = Path(sys.argv[0]).name.strip()
    if (
        not name
        or name in {"python", "python3", "pytest", "py.test"}
        or name.endswith(".py")
        or "pytest" in name
    ):
        return "retrain"
    return name


def _manual_path() -> Path:
    """Location of the editable bundled manual file."""
    return Path(__file__).with_name("retrain.man")


def _load_manual_text(cli_name: str) -> str:
    """Load manual text and substitute runtime command name."""
    path = _manual_path()
    if not path.is_file():
        # Backward-compat fallback for older installs.
        legacy = Path(__file__).with_name("vauban.man")
        if legacy.is_file():
            path = legacy
    if path.is_file():
        text = path.read_text()
    else:
        text = (
            "RETRAIN(1)\n\nNAME\n"
            "    retrain - manual file missing (reinstall package)\n"
        )
    return text.replace("{{CLI}}", cli_name).rstrip() + "\n"


def _is_manual_heading(line: str) -> bool:
    s = line.strip()
    if not s or s != line:
        return False
    allowed = set("ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 _-()")
    return all(ch in allowed for ch in s) and any(ch.isalpha() for ch in s)


def _extract_manual_section(manual_text: str, heading: str) -> str | None:
    """Extract one heading section from manual text."""
    wanted = heading.strip().upper()
    lines = manual_text.splitlines()

    start = None
    for i, line in enumerate(lines):
        if _is_manual_heading(line) and line.strip().upper() == wanted:
            start = i
            break
    if start is None:
        return None

    end = len(lines)
    for i in range(start + 1, len(lines)):
        if _is_manual_heading(lines[i]) and lines[i].strip().upper() != wanted:
            end = i
            break
    return "\n".join(lines[start:end]).rstrip() + "\n"


def _render_commands_block(cli_name: str) -> list[str]:
    return [
        f"    {cli_name} [config.toml]",
        "        Run one training job.",
        "        If config.toml is omitted, uses ./retrain.toml when present.",
        "",
        f"    {cli_name} campaign.toml",
        "        Runs campaign mode when [campaign] exists in TOML.",
        "        Generates conditions x seeds matrix of training runs.",
        "        Set parallel = true in [campaign] for concurrent execution.",
        "        max_workers limits concurrent subprocess count.",
        "",
        f"    {cli_name} squeeze.toml",
        "        Runs squeeze mode when [squeeze] exists in TOML.",
        "        Analyzes LoRA rank via SVD and optionally compresses.",
        "",
        f"    {cli_name} doctor",
        "        Checks optional dependencies for configured components.",
        "",
        f"    {cli_name} init [--template NAME] [--list] [--interactive]",
        "        Writes a starter config in the current directory.",
        "        Templates: default, quickstart, experiment, campaign.",
        "        --template NAME   selects a template (default: default).",
        "        --list            shows available templates.",
        "        --interactive/-i  guided setup with prompts.",
        "",
        f"    {cli_name} status [logdir] [--json]",
        "        Scans log directories for run and campaign status.",
        "        Defaults to ./logs when no path is given.",
        "        --json            machine-readable JSON output.",
        "",
        f"    {cli_name} explain [config.toml] [--json]",
        "        Dry-run preview: shows what a config would do.",
        "        Works for single runs, campaigns, and squeeze configs.",
        "        --json            machine-readable JSON output.",
        "",
        f"    {cli_name} diff <run_a> <run_b> [--json]",
        "        Compares metrics between two training runs.",
        f"    {cli_name} diff <campaign_dir> <cond_a> <cond_b> [--json]",
        "        Compares two conditions in a campaign (averaged across seeds).",
        "        --json            machine-readable JSON output.",
        "",
        f"    {cli_name} man",
        "        Shows this manual.",
        "        --topic <name>    prints one section.",
        "        --path            prints the manual file path.",
        "        --list-topics     lists supported topic names.",
        "        --sync            refreshes auto-generated manual blocks.",
        "        --check           exits non-zero if auto blocks are stale.",
        "        --json            outputs JSON (full manual or single topic).",
    ]


def _render_options_block() -> list[str]:
    from retrain.config import _CLI_FLAG_MAP

    # Keep only canonical flags for readability; skip alias here and add below.
    canonical = sorted(k for k in _CLI_FLAG_MAP if k != "--resume")
    lines = [
        "    Every TrainConfig field is exposed as a --kebab-case CLI flag.",
        "    Flags override TOML values.",
        "",
        "    Common examples:",
        "        retrain config.toml --seed 42 --max-steps 50",
        "        retrain config.toml --lr 1e-4 --batch-size 16",
        "        retrain config.toml --advantage-mode grpo",
        "        retrain config.toml --inference-engine vllm --inference-url http://localhost:8000",
        "",
        "    All flags (sorted):",
    ]
    for flag in canonical:
        lines.append(f"        {flag}")

    lines.extend(
        [
            "",
            "    Special alias:",
            "        --resume VALUE    alias for --resume-from VALUE",
            "",
            "    Unknown flags produce an error with close-match suggestions.",
        ]
    )
    return lines


def _render_quickstart_block(cli_name: str) -> list[str]:
    return [
        "    cp retrain.toml my_run.toml",
        f"    {cli_name} my_run.toml",
        f"    {cli_name} my_run.toml --seed 42 --max-steps 50",
    ]


def _render_environment_block(cli_name: str) -> list[str]:
    from retrain.verifiers_bridge import _FALLBACK_TRAINING_ENVS

    lines = [
        f"    {cli_name} uses verifiers environments for RLVR training data.",
        "    Set [environment].provider = \"verifiers\" and specify a Hub ID.",
        "",
        "    Trainable verifiers examples:",
    ]
    for env_id in _FALLBACK_TRAINING_ENVS:
        lines.append(f"        {env_id}")
    lines.extend(
        [
            "",
            "    Caveat:",
            "        Some hub environments are eval-only and do not expose training",
            f"        datasets. In that case {cli_name} fails fast with actionable guidance.",
        ]
    )
    return lines


def _replace_auto_block(text: str, name: str, rendered_lines: list[str]) -> str:
    start = f"<<AUTO:{name}>>"
    end = f"<<END:AUTO:{name}>>"
    lines = text.splitlines()
    out: list[str] = []
    i = 0
    replaced = False
    while i < len(lines):
        line = lines[i]
        if line.strip() == start:
            replaced = True
            out.append(line)
            out.extend(rendered_lines)
            i += 1
            while i < len(lines) and lines[i].strip() != end:
                i += 1
            if i >= len(lines):
                raise ValueError(f"Missing block end marker: {end}")
            out.append(lines[i])
            i += 1
            continue
        out.append(line)
        i += 1

    if not replaced:
        raise ValueError(f"Missing block markers for {name}: {start} ... {end}")

    return "\n".join(out).rstrip() + "\n"


def _sync_manual_file(cli_name: str) -> tuple[Path, bool]:
    """Refresh auto-generated blocks in the editable manual."""
    path = _manual_path()
    original = path.read_text() if path.is_file() else _load_manual_text(cli_name)

    updated = original
    rendered = {
        "COMMANDS": _render_commands_block(cli_name),
        "OPTIONS": _render_options_block(),
        "QUICKSTART": _render_quickstart_block(cli_name),
        "ENVIRONMENT": _render_environment_block(cli_name),
    }
    for name in _AUTO_BLOCK_NAMES:
        updated = _replace_auto_block(updated, name, rendered[name])

    changed = updated != original
    if changed:
        path.write_text(updated)
    return path, changed


def _check_manual_file(cli_name: str) -> tuple[Path, bool]:
    """Check whether auto-generated manual blocks are up to date."""
    path = _manual_path()
    original = path.read_text() if path.is_file() else _load_manual_text(cli_name)

    updated = original
    rendered = {
        "COMMANDS": _render_commands_block(cli_name),
        "OPTIONS": _render_options_block(),
        "QUICKSTART": _render_quickstart_block(cli_name),
        "ENVIRONMENT": _render_environment_block(cli_name),
    }
    for name in _AUTO_BLOCK_NAMES:
        updated = _replace_auto_block(updated, name, rendered[name])
    return path, updated == original


def _run_man(args: list[str]) -> None:
    """Print manual text (or JSON view) from bundled editable file."""
    fmt = "text"
    topic: str | None = None
    show_path = False
    list_topics = False
    sync = False
    check = False
    i = 0
    while i < len(args):
        arg = args[i]
        if arg == "--json":
            fmt = "json"
        elif arg == "--path":
            show_path = True
        elif arg == "--list-topics":
            list_topics = True
        elif arg == "--sync":
            sync = True
        elif arg == "--check":
            check = True
        elif arg in ("--format", "-f"):
            i += 1
            if i >= len(args):
                print("Flag --format requires a value: text|json", file=sys.stderr)
                sys.exit(1)
            fmt = args[i]
        elif arg.startswith("--format="):
            fmt = arg.split("=", 1)[1]
        elif arg in ("--topic", "-t"):
            i += 1
            if i >= len(args):
                print("Flag --topic requires a value.", file=sys.stderr)
                sys.exit(1)
            topic = args[i]
        elif arg.startswith("--topic="):
            topic = arg.split("=", 1)[1]
        else:
            print(f"Unknown man flag: {arg}", file=sys.stderr)
            sys.exit(1)
        i += 1

    if fmt not in ("text", "json", "troff", "html"):
        print(f"Unsupported format '{fmt}'. Use text|json|troff|html.", file=sys.stderr)
        sys.exit(1)

    cli_name = _resolve_cli_name()
    if sync and check:
        print("Flags --sync and --check cannot be used together.", file=sys.stderr)
        sys.exit(1)

    manual_path = _manual_path()
    if sync:
        try:
            manual_path, changed = _sync_manual_file(cli_name)
        except ValueError as exc:
            print(f"Manual sync failed: {exc}", file=sys.stderr)
            sys.exit(1)
        status = "updated" if changed else "already up to date"
        print(f"{manual_path} ({status})")
        return

    if check:
        try:
            manual_path, up_to_date = _check_manual_file(cli_name)
        except ValueError as exc:
            print(f"Manual check failed: {exc}", file=sys.stderr)
            sys.exit(1)
        if up_to_date:
            print(f"{manual_path} (up to date)")
            return
        print(
            f"{manual_path} (out of date). Run: {cli_name} man --sync",
            file=sys.stderr,
        )
        sys.exit(1)

    manual_text = _load_manual_text(cli_name)

    if show_path:
        print(str(manual_path))
        return

    if list_topics:
        for name in sorted(_TOPIC_TO_SECTION):
            print(name)
        return

    section_name: str | None = None
    section_text: str | None = None
    if topic is not None:
        section_name = _TOPIC_TO_SECTION.get(topic.lower(), topic.upper())
        section_text = _extract_manual_section(manual_text, section_name)
        if section_text is None:
            print(
                f"Unknown topic '{topic}'. Available: {sorted(_TOPIC_TO_SECTION)}",
                file=sys.stderr,
            )
            sys.exit(1)

    if fmt in ("troff", "html"):
        from retrain.man_export import parse_manual, to_html, to_troff

        source = section_text if section_text is not None else manual_text
        sections = parse_manual(source)
        formatter = to_troff if fmt == "troff" else to_html
        print(formatter(sections).rstrip())
        return

    if fmt == "json":
        if section_text is None:
            payload = {
                "tool": cli_name,
                "path": str(manual_path),
                "topics": sorted(_TOPIC_TO_SECTION),
                "manual": manual_text,
            }
        else:
            payload = {
                "tool": cli_name,
                "path": str(manual_path),
                "topic": topic,
                "section": section_name,
                "content": section_text,
            }
        print(json.dumps(payload, indent=2))
        return

    if section_text is None:
        print(manual_text.rstrip())
    else:
        print(section_text.rstrip())


def _customize_toml(
    content: str,
    max_steps: int | None = None,
    seed: int | None = None,
    wandb_project: str | None = None,
) -> str:
    """Apply customizations to a TOML template string via regex."""
    if max_steps is not None:
        content = re.sub(r"^(max_steps\s*=\s*)\d+", rf"\g<1>{max_steps}", content, flags=re.MULTILINE)
    if seed is not None:
        content = re.sub(r"^(seed\s*=\s*)-?\d+", rf"\g<1>{seed}", content, flags=re.MULTILINE)
    if wandb_project is not None:
        if wandb_project:
            # Uncomment and set
            content = re.sub(
                r"^#\s*wandb_project\s*=.*$",
                f'wandb_project = "{wandb_project}"',
                content,
                flags=re.MULTILINE,
            )
            # Replace already-uncommented line
            content = re.sub(
                r'^wandb_project\s*=\s*"[^"]*"',
                f'wandb_project = "{wandb_project}"',
                content,
                flags=re.MULTILINE,
            )
        # Empty string: leave as-is (commented or empty)
    return content


_INTERACTIVE_GOALS: dict[str, tuple[str, int]] = {
    "1": ("quickstart", 20),
    "2": ("experiment", 500),
    "3": ("campaign", 200),
}


def _run_init_interactive(cli_name: str) -> None:
    """Interactively build a retrain config file."""
    if not sys.stdin.isatty():
        print("Interactive init requires a terminal (TTY).", file=sys.stderr)
        sys.exit(1)

    print(f"{cli_name} init — interactive setup\n")
    print("What would you like to do?")
    print("  1) quickstart  — 20-step smoke test")
    print("  2) experiment  — reproducible training run")
    print("  3) campaign    — sweep across conditions and seeds")
    choice = input("\nChoice [1]: ").strip() or "1"
    if choice not in _INTERACTIVE_GOALS:
        print(f"Invalid choice '{choice}'. Using quickstart.", file=sys.stderr)
        choice = "1"

    goal_name, default_steps = _INTERACTIVE_GOALS[choice]

    # Steps
    steps_input = input(f"Training steps [{default_steps}]: ").strip()
    if steps_input:
        try:
            max_steps = int(steps_input)
        except ValueError:
            print(f"Not a number, using default ({default_steps}).", file=sys.stderr)
            max_steps = default_steps
    else:
        max_steps = default_steps

    # Seed
    seed_input = input("Reproducible seed [42]: ").strip()
    if seed_input:
        try:
            seed = int(seed_input)
        except ValueError:
            print("Not a number, using default (42).", file=sys.stderr)
            seed = 42
    else:
        seed = 42

    # Wandb
    wandb_project = input("Wandb project name (empty to skip): ").strip()

    content, filename = _INIT_TEMPLATES[goal_name]
    content = _customize_toml(content, max_steps=max_steps, seed=seed, wandb_project=wandb_project or None)

    dest = Path(filename)
    if dest.exists():
        print(f"{filename} already exists — refusing to overwrite.")
        sys.exit(1)
    dest.write_text(content)
    print(f"\nCreated {filename} (template: {goal_name})")
    print(f"Edit it, then run: {cli_name}")


def _run_init(args: list[str] | None = None, cli_name: str | None = None) -> None:
    """Generate a starter config file in the current directory."""
    if not cli_name:
        cli_name = _resolve_cli_name()
    args = args or []

    template_name = "default"
    list_templates = False
    interactive = False
    i = 0
    while i < len(args):
        arg = args[i]
        if arg in ("--interactive", "-i"):
            interactive = True
        elif arg in ("--list", "-l"):
            list_templates = True
        elif arg in ("--template", "-t"):
            i += 1
            if i >= len(args):
                print("Flag --template requires a value.", file=sys.stderr)
                sys.exit(1)
            template_name = args[i]
        elif arg.startswith("--template="):
            template_name = arg.split("=", 1)[1]
        else:
            print(f"Unknown init flag: {arg}", file=sys.stderr)
            sys.exit(1)
        i += 1

    if list_templates:
        print("Available templates:")
        for name, (_, filename) in sorted(_INIT_TEMPLATES.items()):
            print(f"  {name:12s} -> {filename}")
        return

    if interactive:
        _run_init_interactive(cli_name)
        return

    if template_name not in _INIT_TEMPLATES:
        print(
            f"Unknown template '{template_name}'. "
            f"Available: {sorted(_INIT_TEMPLATES)}",
            file=sys.stderr,
        )
        sys.exit(1)

    content, filename = _INIT_TEMPLATES[template_name]
    dest = Path(filename)
    if dest.exists():
        print(f"{filename} already exists — refusing to overwrite.")
        sys.exit(1)
    dest.write_text(content)
    print(f"Created {filename} (template: {template_name})")
    print(f"Edit it, then run: {cli_name}")
    print(f"Need guidance? Run: {cli_name} man")


def _load_dotenv() -> None:
    """Load .env file if present. Sets vars into os.environ."""
    env_path = Path(".env")
    if not env_path.exists():
        return
    for line in env_path.read_text().splitlines():
        line = line.strip()
        if not line or line.startswith("#"):
            continue
        eq = line.find("=")
        if eq == -1:
            continue
        key = line[:eq].strip()
        val = line[eq + 1 :].strip()
        if len(val) >= 2 and val[0] == val[-1] and val[0] in ('"', "'"):
            val = val[1:-1]
        os.environ[key] = val
    print("Loaded .env")


def _is_squeeze(path: str) -> bool:
    """Check if a TOML file has a [squeeze] section."""
    with open(path, "rb") as f:
        data = tomllib.load(f)
    return "squeeze" in data


def _is_campaign(path: str) -> bool:
    """Check if a TOML file has a [campaign] section."""
    with open(path, "rb") as f:
        data = tomllib.load(f)
    return "campaign" in data


def _run_doctor() -> None:
    """Print dependency status for all known components."""
    from retrain.registry import check_environment

    print("retrain doctor — checking component dependencies\n")
    results = check_environment(config=None)
    all_ok = True
    for name, import_name, hint, available in results:
        status = "OK" if available else "MISSING"
        if not available:
            all_ok = False
        print(f"  {name:20s} {import_name:25s} {status}")
        if not available:
            print(f"  {'':20s} -> {hint}")
    print()
    if all_ok:
        print("All optional dependencies are installed.")
    else:
        print("Some optional dependencies are missing (see above).")


def _check_environment(config: "TrainConfig") -> None:  # noqa: F821
    """Warn if the config references components whose deps are missing."""
    from retrain.registry import check_environment

    results = check_environment(config=config)
    for name, import_name, hint, available in results:
        if not available:
            print(
                f"WARNING: component '{name}' requires '{import_name}' "
                f"which is not installed.\n  -> {hint}"
            )


def _run_status(args: list[str]) -> None:
    """Scan log directories and print run/campaign status."""
    from retrain.status import (
        format_campaign,
        format_run,
        scan_all,
    )

    fmt = "text"
    root = "logs"
    for arg in args:
        if arg == "--json":
            fmt = "json"
        elif arg.startswith("--"):
            print(f"Unknown status flag: {arg}", file=sys.stderr)
            sys.exit(1)
        else:
            root = arg

    root_path = Path(root)
    if not root_path.is_dir():
        print(f"No log directory found: {root}")
        sys.exit(1)

    runs, campaigns = scan_all(root_path)

    if fmt == "json":
        payload = {
            "root": str(root_path),
            "runs": [r.to_dict() for r in runs],
            "campaigns": [c.to_dict() for c in campaigns],
        }
        print(json.dumps(payload, indent=2))
        return

    if not runs and not campaigns:
        print(f"No runs or campaigns found in {root}")
        return

    if campaigns:
        for camp in campaigns:
            print(format_campaign(camp))
            print()

    if runs:
        print("Standalone runs:")
        for run in runs:
            print(format_run(run))


def _explain_single(config_path: str | None, fmt: str) -> None:
    """Explain what a single training run would do."""
    import warnings

    from retrain.config import load_config
    from retrain.registry import check_environment

    with warnings.catch_warnings(record=True) as caught:
        warnings.simplefilter("always")
        config = load_config(config_path)

    condition = f"{config.advantage_mode}+{config.transform_mode}"
    datums_per_step = config.batch_size * config.group_size
    total_datums = datums_per_step * config.max_steps
    lora_alpha = config.lora_alpha if config.lora_alpha else config.lora_rank * 2
    data_info = config.data_source
    if config.environment_provider:
        data_info = f"{config.environment_provider}:{config.environment_id}"

    info: dict = {
        "mode": "single",
        "config": config_path or "retrain.toml",
        "model": config.model,
        "condition": condition,
        "advantage_mode": config.advantage_mode,
        "transform_mode": config.transform_mode,
        "max_steps": config.max_steps,
        "batch_size": config.batch_size,
        "group_size": config.group_size,
        "datums_per_step": datums_per_step,
        "total_datums": total_datums,
        "max_tokens": config.max_tokens,
        "temperature": config.temperature,
        "lr": config.lr,
        "seed": config.seed,
        "lora_rank": config.lora_rank,
        "lora_alpha": lora_alpha,
        "data": data_info,
        "reward_type": config.reward_type,
        "log_dir": config.log_dir,
        "adapter_path": config.adapter_path,
        "wandb_project": config.wandb_project or "(disabled)",
        "warnings": [str(w.message) for w in caught],
    }

    # Dependency warnings
    dep_warnings = []
    results = check_environment(config=config)
    for name, import_name, hint, available in results:
        if not available:
            dep_warnings.append(f"{name} requires {import_name} ({hint})")
    if dep_warnings:
        info["dep_warnings"] = dep_warnings

    if fmt == "json":
        print(json.dumps(info, indent=2))
        return

    print(f"retrain explain — dry-run preview")
    print(f"  config        : {info['config']}")
    print(f"  model         : {config.model}")
    print(f"  condition     : {condition}")
    print(f"  steps         : {config.max_steps}")
    print(f"  batch_size    : {config.batch_size}")
    print(f"  group_size    : {config.group_size}")
    print(f"  datums/step   : {datums_per_step}")
    print(f"  total datums  : {total_datums}")
    print(f"  max_tokens    : {config.max_tokens}")
    print(f"  temperature   : {config.temperature}")
    print(f"  lr            : {config.lr}")
    print(f"  seed          : {config.seed}")
    print(f"  lora          : rank={config.lora_rank} alpha={lora_alpha}")
    print(f"  data          : {data_info}")
    print(f"  reward        : {config.reward_type}")
    print(f"  log_dir       : {config.log_dir}")
    print(f"  adapter_path  : {config.adapter_path}")
    print(f"  wandb         : {info['wandb_project']}")
    if caught:
        print("\nWarnings:")
        for w in caught:
            print(f"  - {w.message}")
    if dep_warnings:
        print("\nMissing dependencies:")
        for dw in dep_warnings:
            print(f"  - {dw}")


def _explain_campaign(config_path: str, fmt: str) -> None:
    """Explain what a campaign would do."""
    from retrain.campaign import DEFAULT_SEEDS, _parse_campaign_conditions

    with open(config_path, "rb") as f:
        data = tomllib.load(f)

    campaign = data.get("campaign", {})
    seeds = campaign.get("seeds", DEFAULT_SEEDS)
    max_steps = campaign.get("max_steps", 500)
    raw_conditions = campaign.get("conditions", None)
    conditions = _parse_campaign_conditions(raw_conditions, config_path)
    condition_labels = [f"{a}+{t}" for a, t in conditions]
    total_runs = len(conditions) * len(seeds)

    info = {
        "mode": "campaign",
        "config": config_path,
        "conditions": condition_labels,
        "seeds": seeds,
        "max_steps": max_steps,
        "total_runs": total_runs,
    }

    if fmt == "json":
        print(json.dumps(info, indent=2))
        return

    print(f"retrain explain — campaign dry-run preview")
    print(f"  config        : {config_path}")
    print(f"  conditions    : {', '.join(condition_labels)}")
    print(f"  seeds         : {seeds}")
    print(f"  max_steps     : {max_steps}")
    print(f"  total runs    : {total_runs}")


def _explain_squeeze(config_path: str, fmt: str) -> None:
    """Explain what a squeeze run would do."""
    from retrain.config import load_squeeze_config

    cfg = load_squeeze_config(config_path)

    info = {
        "mode": "squeeze",
        "config": config_path,
        "adapter_path": cfg.adapter_path,
        "source_rank": cfg.source_rank,
        "min_variance_retention": cfg.min_variance_retention,
    }
    if cfg.output_path:
        info["output_path"] = cfg.output_path
    if cfg.compress_to > 0:
        info["compress_to"] = cfg.compress_to

    if fmt == "json":
        print(json.dumps(info, indent=2))
        return

    print(f"retrain explain — squeeze dry-run preview")
    print(f"  config                : {config_path}")
    print(f"  adapter_path          : {cfg.adapter_path}")
    print(f"  source_rank           : {cfg.source_rank}")
    print(f"  min_variance_retention: {cfg.min_variance_retention}")
    if cfg.output_path:
        print(f"  output_path           : {cfg.output_path}")
    if cfg.compress_to > 0:
        print(f"  compress_to           : {cfg.compress_to}")


def _run_diff(args: list[str]) -> None:
    """Compare two runs or campaign conditions."""
    from retrain.diff import diff_conditions, diff_runs, format_diff

    fmt = "text"
    positional: list[str] = []
    for arg in args:
        if arg == "--json":
            fmt = "json"
        elif arg.startswith("--"):
            print(f"Unknown diff flag: {arg}", file=sys.stderr)
            sys.exit(1)
        else:
            positional.append(arg)

    if len(positional) == 2:
        dir_a, dir_b = Path(positional[0]), Path(positional[1])
        try:
            result = diff_runs(dir_a, dir_b)
        except FileNotFoundError as exc:
            print(str(exc), file=sys.stderr)
            sys.exit(1)
    elif len(positional) == 3:
        campaign_dir = Path(positional[0])
        cond_a, cond_b = positional[1], positional[2]
        try:
            result = diff_conditions(campaign_dir, cond_a, cond_b)
        except FileNotFoundError as exc:
            print(str(exc), file=sys.stderr)
            sys.exit(1)
    else:
        print("Usage:", file=sys.stderr)
        print("  retrain diff <run_a> <run_b>", file=sys.stderr)
        print("  retrain diff <campaign_dir> <cond_a> <cond_b>", file=sys.stderr)
        sys.exit(1)

    if fmt == "json":
        from dataclasses import asdict

        print(json.dumps(asdict(result), indent=2))
    else:
        print(format_diff(result))


def _run_explain(args: list[str]) -> None:
    """Dry-run: show what a config would do without running it."""
    fmt = "text"
    config_path: str | None = None
    for arg in args:
        if arg == "--json":
            fmt = "json"
        elif arg.startswith("--"):
            print(f"Unknown explain flag: {arg}", file=sys.stderr)
            sys.exit(1)
        else:
            config_path = arg

    # Resolve config path
    if config_path is None:
        if Path("retrain.toml").is_file():
            config_path = "retrain.toml"
        else:
            print("No config file specified and no retrain.toml in cwd.")
            sys.exit(1)

    if not Path(config_path).is_file():
        print(f"File not found: {config_path}")
        sys.exit(1)

    # Route by config type
    if _is_campaign(config_path):
        _explain_campaign(config_path, fmt)
    elif _is_squeeze(config_path):
        _explain_squeeze(config_path, fmt)
    else:
        _explain_single(config_path, fmt)


def main() -> None:
    """Single entry point: retrain config.toml"""
    _load_dotenv()

    args = sys.argv[1:]
    cli_name = _resolve_cli_name()

    if args and args[0] in ("-h", "--help", "help"):
        _print_top_help(cli_name)
        sys.exit(0)

    if args and args[0] in ("man", "manual"):
        _run_man(args[1:])
        sys.exit(0)

    if args and args[0] == "doctor":
        _run_doctor()
        sys.exit(0)

    if args and args[0] == "init":
        _run_init(args=args[1:], cli_name=cli_name)
        sys.exit(0)

    if args and args[0] == "status":
        _run_status(args[1:])
        sys.exit(0)

    if args and args[0] == "explain":
        _run_explain(args[1:])
        sys.exit(0)

    if args and args[0] == "diff":
        _run_diff(args[1:])
        sys.exit(0)

    # Parse CLI overrides
    from retrain.config import parse_cli_overrides

    config_path, overrides = parse_cli_overrides(args)

    # Resolve config path
    if config_path is None:
        if Path("retrain.toml").is_file():
            config_path = "retrain.toml"
        elif not overrides:
            if sys.stdin.isatty():
                answer = input("No retrain.toml found. Create one now? [Y/n]: ").strip().lower()
                if answer in ("", "y", "yes"):
                    _run_init(cli_name=cli_name)
                    sys.exit(0)
            print("No retrain.toml found. Create one with:")
            print(f"  {cli_name} init")
            print("Or pass a path:")
            print(f"  {cli_name} path/to/config.toml")
            print("Manual:")
            print(f"  {cli_name} man")
            sys.exit(1)
        # else: overrides-only mode, use defaults

    if config_path is not None and not Path(config_path).is_file():
        print(f"File not found: {config_path}")
        sys.exit(1)

    # Route: campaign | squeeze | single run
    # Campaign/squeeze only when a TOML file is provided (CLI overrides don't apply)
    if config_path is not None and _is_campaign(config_path):
        from retrain.campaign import run_campaign
        run_campaign(config_path)
    elif config_path is not None and _is_squeeze(config_path):
        from retrain.squeeze import run_squeeze
        run_squeeze(config_path)
    else:
        from retrain.config import load_config
        from retrain.trainer import train
        config = load_config(config_path, overrides=overrides)
        _check_environment(config)
        train(config)


if __name__ == "__main__":
    main()
